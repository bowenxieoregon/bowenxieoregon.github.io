---
date: '2026-01-04'
draft: false
title: 'Building an LLM from Scratch: CS336 Assignment 1'
description: 'My implementation journey through Stanford CS336 Assignment 1 - building language model components from the ground up.'
tags: ['CS336', 'LLM', 'Deep Learning', 'Transformers', 'NLP']
math: true
comments: true
---

This post documents my journey through **Stanford CS336: Language Models from Scratch** â€” specifically Assignment 1, where we implement core components of a language model.

## Overview

CS336 is Stanford's deep dive into building large language models from first principles. Assignment 1 focuses on:

- Tokenization (BPE implementation)
- Transformer architecture components
- Training loop fundamentals

## Key Takeaways

### 1. Byte-Pair Encoding (BPE)

*Coming soon...*

### 2. Attention Mechanism

*Coming soon...*

### 3. Training Considerations

*Coming soon...*

## Implementation Highlights

```python
# Code snippets coming soon...
```

## Challenges & Lessons Learned

*Coming soon...*

## References

1. [Stanford CS336 Course Page](https://stanford-cs336.github.io/)
2. Vaswani et al., "Attention Is All You Need" (2017)
3. Sennrich et al., "Neural Machine Translation of Rare Words with Subword Units" (2016)

---

*This post is a work in progress. Check back for updates!*
